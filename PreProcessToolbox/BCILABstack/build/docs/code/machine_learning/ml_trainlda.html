<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainlda</title>
  <meta name="keywords" content="ml_trainlda">
  <meta name="description" content="Learn a linear predictive model by (regularized) Linear Discriminant Analysis.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainlda.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainlda
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a linear predictive model by (regularized) Linear Discriminant Analysis.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainlda(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a linear predictive model by (regularized) Linear Discriminant Analysis.
 Model = ml_trainlda(Trials, Targets, Lambda, Options...)

 LDA is one of the simplest and oldest learning algorithms, originally introduced by Fisher in [1].
 Its basic assumption is that the data originates from two classes (extended to more via 1-vs-1
 voting), where the data in each class is distributed in the feature space according to a normal
 distribution. The shape of the distribution is assumed to be identical for both classes, and the
 relative weight/prior probability of each class is (by default) assumed to be identical, as well.
 An example would be data with features generated as a weighted sum of some latent variables which
 take on different values between conditions (but identical values within each condition), and
 which are superimposed with gaussian noise (which is identically distributed across conditions,
 e.g. from a large sum of independent random variables).

 Despite its simplicity, LDA assumes more structure in the data than is usually necessary (namely a
 certain normal distribution per class), and sometimes, more than what can be satisfactorily
 learned from the data, so that, even when the assumptions are fulfilled, the method is not
 guaranteed to give the optimal result. The estimation of the per-class covariance matrix is a
 notoriously data-hungry (and especially outlier-sensitive) step, and the main weakness of standard
 LDA.

 ml_trainlda offers three advanced covariance estimators, with different trade-offs to mitigate
 these problems. The variants 'shrinkage' and 'independence' each introduce a regularization
 parameter [2] which controls the complexity of the estimated matrix, and which need to be selected
 in a parameter search (which is orders of magnitude more time-consuming). The 'auto' variant
 computes the degree of regularization analyically in closed form, and is therefore both fast and
 (in some sense) optimal (but more restricted than 'independence') [3]. When enough trials are
 available, full covariance matrices are learned, but the less trials are given, the more the
 covariance estimates degrade to spherical (though well-formed) ones. Auto is the default for lda.

 While regularization can automatically control the complexity of a classifier, it is not a panacea
 which allows to add arbitrarily many features, since with each additional feature, the amount of
 structure (here:correlation) that can be captured in the remaining ones gets reduced.

 Within the toolbox, LDA is one of the bread-and-butter classifiers, and is worth trying in every
 reasonably simple classification task, especially for its speed. The major problem with LDA
 compared to other available classifiers is that it easier to break it with outliers than, for
 example, support vector machines or logistic regression. Another weakness is that the outputs are
 relatively primitive probability estimates, in contrast to, for example, logistic regression
 (somewhat better) or relevance vector machines (clearly better).

 In:
   Trials       : training data, as in ml_train

   Targets      : target variable, as in ml_train

   Lambda       : optional regularization parameter, reasonable range: 0:0.1:1, greater is stronger
                  requires that the regularization mode is set to either 'shrinkage' or 'independence' (default: [])
           
   Options  : optional name-value parameters to control the training details:
              'regularization' -&gt; 'shrinkage': covariance shrinkage, depends on plambda 
                                  'independence': feature independence, depends on plambda
                                  'auto': analytical covariance shrinkage, plambda is ignored (default)
              'weight_bias' -&gt; 0/1, take unequal class priors into account for bias calculation
                               default: 0
              'weight_cov' -&gt; 0/1, take unequal class priors into account for covariance calculation
                              default: 0
 Out:
   Model   : a linear model; w is the linear weights, b is the bias; classes indicates the class labels which the model predicts

 Examples:
   % learn a standard shrinkage LDA model
   model = ml_trainlda(trials,targets);

   % take unequal class priors into account for both the bias and the covariance matrix
   model = ml_trainlda(trials,targets,[],'weight_bias',1,'weight_cov',1);

   % use a different type of regularization, which controls feature independence and requires cross-validation
   model = utl_searchmodel({trials,target},'args',{{'lda',search(0:0.1:1),'regularization','independence'}})


 See also:
   <a href="ml_predictlda.html" class="code" title="function pred = ml_predictlda(trials, model)">ml_predictlda</a>

 References:
   [1] Fisher, R. &quot;The use of multiple measurements in taxonomic problems.&quot;
       Annals Eugen. 7 (1936), 188, 179.
   [2] Friedman, J. &quot;Regularized discriminant analysis.&quot;
       Journal of the American Statistical Association 84, 405 (1989), 175, 165.
   [3] O. Ledoit and M. Wolf, &quot;A well-conditioned estimator for large-dimensional covariance matrices&quot;
       J Multivar Anal, 88(2): 365-411, 2004.

                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                           2010-04-03</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_predictlda.html" class="code" title="function pred = ml_predictlda(trials, model)">ml_predictlda</a>	Prediction function for Linear Discriminant Analysis</li><li><a href="ml_trainlda.html" class="code" title="function model = ml_trainlda(varargin)">ml_trainlda</a>	Learn a linear predictive model by (regularized) Linear Discriminant Analysis.</li><li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_trainlda.html" class="code" title="function model = ml_trainlda(varargin)">ml_trainlda</a>	Learn a linear predictive model by (regularized) Linear Discriminant Analysis.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainlda(varargin)</a>
0002 <span class="comment">% Learn a linear predictive model by (regularized) Linear Discriminant Analysis.</span>
0003 <span class="comment">% Model = ml_trainlda(Trials, Targets, Lambda, Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% LDA is one of the simplest and oldest learning algorithms, originally introduced by Fisher in [1].</span>
0006 <span class="comment">% Its basic assumption is that the data originates from two classes (extended to more via 1-vs-1</span>
0007 <span class="comment">% voting), where the data in each class is distributed in the feature space according to a normal</span>
0008 <span class="comment">% distribution. The shape of the distribution is assumed to be identical for both classes, and the</span>
0009 <span class="comment">% relative weight/prior probability of each class is (by default) assumed to be identical, as well.</span>
0010 <span class="comment">% An example would be data with features generated as a weighted sum of some latent variables which</span>
0011 <span class="comment">% take on different values between conditions (but identical values within each condition), and</span>
0012 <span class="comment">% which are superimposed with gaussian noise (which is identically distributed across conditions,</span>
0013 <span class="comment">% e.g. from a large sum of independent random variables).</span>
0014 <span class="comment">%</span>
0015 <span class="comment">% Despite its simplicity, LDA assumes more structure in the data than is usually necessary (namely a</span>
0016 <span class="comment">% certain normal distribution per class), and sometimes, more than what can be satisfactorily</span>
0017 <span class="comment">% learned from the data, so that, even when the assumptions are fulfilled, the method is not</span>
0018 <span class="comment">% guaranteed to give the optimal result. The estimation of the per-class covariance matrix is a</span>
0019 <span class="comment">% notoriously data-hungry (and especially outlier-sensitive) step, and the main weakness of standard</span>
0020 <span class="comment">% LDA.</span>
0021 <span class="comment">%</span>
0022 <span class="comment">% ml_trainlda offers three advanced covariance estimators, with different trade-offs to mitigate</span>
0023 <span class="comment">% these problems. The variants 'shrinkage' and 'independence' each introduce a regularization</span>
0024 <span class="comment">% parameter [2] which controls the complexity of the estimated matrix, and which need to be selected</span>
0025 <span class="comment">% in a parameter search (which is orders of magnitude more time-consuming). The 'auto' variant</span>
0026 <span class="comment">% computes the degree of regularization analyically in closed form, and is therefore both fast and</span>
0027 <span class="comment">% (in some sense) optimal (but more restricted than 'independence') [3]. When enough trials are</span>
0028 <span class="comment">% available, full covariance matrices are learned, but the less trials are given, the more the</span>
0029 <span class="comment">% covariance estimates degrade to spherical (though well-formed) ones. Auto is the default for lda.</span>
0030 <span class="comment">%</span>
0031 <span class="comment">% While regularization can automatically control the complexity of a classifier, it is not a panacea</span>
0032 <span class="comment">% which allows to add arbitrarily many features, since with each additional feature, the amount of</span>
0033 <span class="comment">% structure (here:correlation) that can be captured in the remaining ones gets reduced.</span>
0034 <span class="comment">%</span>
0035 <span class="comment">% Within the toolbox, LDA is one of the bread-and-butter classifiers, and is worth trying in every</span>
0036 <span class="comment">% reasonably simple classification task, especially for its speed. The major problem with LDA</span>
0037 <span class="comment">% compared to other available classifiers is that it easier to break it with outliers than, for</span>
0038 <span class="comment">% example, support vector machines or logistic regression. Another weakness is that the outputs are</span>
0039 <span class="comment">% relatively primitive probability estimates, in contrast to, for example, logistic regression</span>
0040 <span class="comment">% (somewhat better) or relevance vector machines (clearly better).</span>
0041 <span class="comment">%</span>
0042 <span class="comment">% In:</span>
0043 <span class="comment">%   Trials       : training data, as in ml_train</span>
0044 <span class="comment">%</span>
0045 <span class="comment">%   Targets      : target variable, as in ml_train</span>
0046 <span class="comment">%</span>
0047 <span class="comment">%   Lambda       : optional regularization parameter, reasonable range: 0:0.1:1, greater is stronger</span>
0048 <span class="comment">%                  requires that the regularization mode is set to either 'shrinkage' or 'independence' (default: [])</span>
0049 <span class="comment">%</span>
0050 <span class="comment">%   Options  : optional name-value parameters to control the training details:</span>
0051 <span class="comment">%              'regularization' -&gt; 'shrinkage': covariance shrinkage, depends on plambda</span>
0052 <span class="comment">%                                  'independence': feature independence, depends on plambda</span>
0053 <span class="comment">%                                  'auto': analytical covariance shrinkage, plambda is ignored (default)</span>
0054 <span class="comment">%              'weight_bias' -&gt; 0/1, take unequal class priors into account for bias calculation</span>
0055 <span class="comment">%                               default: 0</span>
0056 <span class="comment">%              'weight_cov' -&gt; 0/1, take unequal class priors into account for covariance calculation</span>
0057 <span class="comment">%                              default: 0</span>
0058 <span class="comment">% Out:</span>
0059 <span class="comment">%   Model   : a linear model; w is the linear weights, b is the bias; classes indicates the class labels which the model predicts</span>
0060 <span class="comment">%</span>
0061 <span class="comment">% Examples:</span>
0062 <span class="comment">%   % learn a standard shrinkage LDA model</span>
0063 <span class="comment">%   model = ml_trainlda(trials,targets);</span>
0064 <span class="comment">%</span>
0065 <span class="comment">%   % take unequal class priors into account for both the bias and the covariance matrix</span>
0066 <span class="comment">%   model = ml_trainlda(trials,targets,[],'weight_bias',1,'weight_cov',1);</span>
0067 <span class="comment">%</span>
0068 <span class="comment">%   % use a different type of regularization, which controls feature independence and requires cross-validation</span>
0069 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'lda',search(0:0.1:1),'regularization','independence'}})</span>
0070 <span class="comment">%</span>
0071 <span class="comment">%</span>
0072 <span class="comment">% See also:</span>
0073 <span class="comment">%   ml_predictlda</span>
0074 <span class="comment">%</span>
0075 <span class="comment">% References:</span>
0076 <span class="comment">%   [1] Fisher, R. &quot;The use of multiple measurements in taxonomic problems.&quot;</span>
0077 <span class="comment">%       Annals Eugen. 7 (1936), 188, 179.</span>
0078 <span class="comment">%   [2] Friedman, J. &quot;Regularized discriminant analysis.&quot;</span>
0079 <span class="comment">%       Journal of the American Statistical Association 84, 405 (1989), 175, 165.</span>
0080 <span class="comment">%   [3] O. Ledoit and M. Wolf, &quot;A well-conditioned estimator for large-dimensional covariance matrices&quot;</span>
0081 <span class="comment">%       J Multivar Anal, 88(2): 365-411, 2004.</span>
0082 <span class="comment">%</span>
0083 <span class="comment">%                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0084 <span class="comment">%                           2010-04-03</span>
0085         
0086 arg_define([0 3],varargin, <span class="keyword">...</span>
0087     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0088     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0089     arg({<span class="string">'plambda'</span>,<span class="string">'Lambda'</span>,<span class="string">'lambda'</span>}, [], [], <span class="string">'Optional regularization parameter. Reasonable range: 0:0.1:1 - greater is stronger. Requires that the regularization mode is set to either &quot;shrinkage&quot; or &quot;independence&quot; (not necessary in &quot;auto&quot; mode).'</span>), <span class="keyword">...</span>
0090     arg({<span class="string">'regularization'</span>,<span class="string">'Regularizer'</span>}, <span class="string">'auto'</span>, {<span class="string">'none'</span>,<span class="string">'auto'</span>,<span class="string">'shrinkage'</span>,<span class="string">'independence'</span>}, <span class="string">'Type of regularization. Regularizes the robustness / flexibility of covariance estimates. Auto is analytical covariance shrinkage, shrinkage is shrinkage as selected via plambda, and independence is feature independence, also selected via plambda.'</span>), <span class="keyword">...</span>
0091     arg({<span class="string">'weight_bias'</span>,<span class="string">'WeightedBias'</span>}, false, [], <span class="string">'Account for class priors in bias. If you do have unequal probabilities for the different classes, this should be enabled.'</span>), <span class="keyword">...</span>
0092     arg({<span class="string">'weight_cov'</span>,<span class="string">'WeightedCov'</span>}, false, [], <span class="string">'Account for class priors in covariance. If you do have unequal probabilities for the different classes, it makes sense to enable this.'</span>));
0093 
0094 <span class="comment">% find the class labels</span>
0095 classes = unique(targets);
0096 <span class="keyword">if</span> length(classes) &gt; 2
0097     <span class="comment">% learn a voting arrangement of models...</span>
0098     model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(trials, targets, <span class="string">'1vR'</span>, @<a href="ml_trainlda.html" class="code" title="function model = ml_trainlda(varargin)">ml_trainlda</a>, @<a href="ml_predictlda.html" class="code" title="function pred = ml_predictlda(trials, model)">ml_predictlda</a>, varargin{:},<span class="string">'weight_bias'</span>,true);    <span class="comment">%#ok&lt;*NODEF&gt;</span>
0099 <span class="keyword">elseif</span> length(classes) == 1
0100     error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0101 <span class="keyword">else</span>
0102     <span class="comment">% pre-prune degenerate features</span>
0103     retain = true(1,size(trials,2));
0104     <span class="keyword">for</span> c = 1:length(classes)
0105         X = trials(targets==classes(c),:);
0106         n{c} = size(X,1);
0107         mu{c} = mean(X,1);
0108         v{c} = var(X,[],1);
0109         retain = retain &amp; isfinite(mu{c}) &amp; isfinite(v{c}) &amp; (n{c}==1 | v{c} &gt; eps);
0110     <span class="keyword">end</span>    
0111     <span class="comment">% apply feature mask...</span>
0112     trials = trials(:,retain);
0113     <span class="comment">% estimate distribution of each class...</span>
0114     <span class="keyword">for</span> c = 1:length(classes)
0115         X = trials(targets==classes(c),:);
0116         n{c} = size(X,1);
0117         mu{c} = mean(X,1);
0118         <span class="keyword">if</span> n{c} == 1
0119             sig{c} = zeros(size(X,2));
0120         <span class="keyword">elseif</span> strcmp(regularization,<span class="string">'auto'</span>)
0121             sig{c} = cov_shrink(X);
0122         <span class="keyword">else</span>
0123             sig{c} = cov(X);
0124             <span class="keyword">if</span> ~isempty(plambda) &amp;&amp; ~strcmp(regularization,<span class="string">'none'</span>)
0125                 <span class="comment">% plambda-dependent regularization</span>
0126                 <span class="keyword">if</span> strcmp(regularization,<span class="string">'independence'</span>)
0127                     sig{c} = (1-plambda)*sig{c} + plambda * diag(diag(sig{c}));
0128                 <span class="keyword">elseif</span> strcmp(regularization,<span class="string">'shrinkage'</span>)
0129                     sig{c} = (1-plambda)*sig{c} + plambda*eye(length(sig{c}))*abs(mean(diag(sig{c})));
0130                 <span class="keyword">else</span>
0131                     error(<span class="string">'unknown regularization mode'</span>);
0132                 <span class="keyword">end</span>
0133             <span class="keyword">end</span>
0134         <span class="keyword">end</span>
0135     <span class="keyword">end</span>
0136     
0137     ns = fastif(weight_cov,n,{1 1});
0138     nb = fastif(weight_bias,n,{1 1});
0139     <span class="comment">% do the math</span>
0140     mu_both = (mu{1}*nb{2} + mu{2}*nb{1}) / (nb{1}+nb{2});    
0141     sig_both = (sig{1}*ns{1} + sig{2}*ns{2}) / (ns{1}+ns{2});
0142     w = (mu{2} - mu{1}) / sig_both;
0143     w = w / (mu{2}*w' - mu_both*w');
0144     model = struct(<span class="string">'w'</span>,{w}, <span class="string">'b'</span>,{mu_both*w'}, <span class="string">'classes'</span>,{classes},<span class="string">'featuremask'</span>,{retain});
0145 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 20-Aug-2013 03:44:10 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>