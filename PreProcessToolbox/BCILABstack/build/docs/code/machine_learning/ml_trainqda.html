<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainqda</title>
  <meta name="keywords" content="ml_trainqda">
  <meta name="description" content="Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainqda.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainqda
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainqda(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.
 Model = ml_trainqda(Trials, Targets, Lambda, Kappa Options...)

 Quadratic discriminant analysis [1] is a (quadratic) generalization of linear discriminant
 analysis, and practically all remarks from ml_trainlda apply to this classifier, as well. However,
 QDA is siginificantly more prone to overfitting than is LDA, since it relaxes the assumption that
 the covariance structure of each class is identical. For this reason, it has twice as many
 parameters, and is also approx. twice as slow. Aside from that, QDA is the simplest non-linear
 classifier and is interesting especially when it is known that one distribution is shaped
 significantly different from the other (e.g. in one-vs-rest votings, or in baseline-versus-signal
 classification).

 In:
   Trials       : training data, as in ml_train

   Targets      : target variable, as in ml_train

   Lambda       : optional per-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger
                  requires that the regularization option is set to either 'shrinkage' or 'independence' (default: [])
   Kappa        : between-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger (default: [])

   Options  : optional name-value parameters to control the training details:
              'weight_bias' -&gt; 0/1, take unequal class priors into account for bias calculation
                               default: 0 -- note: this parameter is currently ignored
              'weight_cov' -&gt; 0/1, take unequal class priors into account for covariance calculation
                              default: 0
              'regularization' -&gt; 'shrinkage': covariance shrinkage, depends on lambda 
                                  'independence': feature independence, depends on lambda
                                  'auto': analytical covariance shrinkage, lambda is ignored (default)
 Out:
   Model   : a quadratic model; q is the quadratic weights, l is the linear weights, b is the bias; 
             classes indicates the class labels which the model predicts; sc_info holds scale parameters

 Examples:
   % learn a shrinkage QDA model
   model = ml_trainqda(trials,targets);

   % take unequal class priors into account for both the bias and the covariance matrix
   model = ml_trainqda(trials,targets,[],[],'weight_bias',1,'weight_cov',1);

   % learn a shrinkage QDA model which blends covariance estimates between classes, using a 
   % regularization parameter
   model = utl_searchmodel({trials,target},'args',{{'qda',[],search(0:0.1:1)}})

   % use a different type of regularization, which controls feature independence
   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),[],'regularization','independence'}})
   
   % as before, but search for the optimal blending parameter at the same time (slow!)
   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),search(0:0.1:1),'regularization','independence'}})

 See also:
   <a href="ml_predictqda.html" class="code" title="function pred = ml_predictqda(trials, model)">ml_predictqda</a>

 References:
   [1] Friedman, J. &quot;Regularized discriminant analysis.&quot;
       Journal of the American Statistical Association 84, 405 (1989), 175, 165.

                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                           2010-04-03</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_predictqda.html" class="code" title="function pred = ml_predictqda(trials, model)">ml_predictqda</a>	Prediction function for Quadratic Discriminant Analysis.</li><li><a href="ml_trainqda.html" class="code" title="function model = ml_trainqda(varargin)">ml_trainqda</a>	Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</li><li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_trainqda.html" class="code" title="function model = ml_trainqda(varargin)">ml_trainqda</a>	Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainqda(varargin)</a>
0002 <span class="comment">% Learn a non-linear predictive model by (regularized) Quadratic Discriminant Analysis.</span>
0003 <span class="comment">% Model = ml_trainqda(Trials, Targets, Lambda, Kappa Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% Quadratic discriminant analysis [1] is a (quadratic) generalization of linear discriminant</span>
0006 <span class="comment">% analysis, and practically all remarks from ml_trainlda apply to this classifier, as well. However,</span>
0007 <span class="comment">% QDA is siginificantly more prone to overfitting than is LDA, since it relaxes the assumption that</span>
0008 <span class="comment">% the covariance structure of each class is identical. For this reason, it has twice as many</span>
0009 <span class="comment">% parameters, and is also approx. twice as slow. Aside from that, QDA is the simplest non-linear</span>
0010 <span class="comment">% classifier and is interesting especially when it is known that one distribution is shaped</span>
0011 <span class="comment">% significantly different from the other (e.g. in one-vs-rest votings, or in baseline-versus-signal</span>
0012 <span class="comment">% classification).</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% In:</span>
0015 <span class="comment">%   Trials       : training data, as in ml_train</span>
0016 <span class="comment">%</span>
0017 <span class="comment">%   Targets      : target variable, as in ml_train</span>
0018 <span class="comment">%</span>
0019 <span class="comment">%   Lambda       : optional per-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger</span>
0020 <span class="comment">%                  requires that the regularization option is set to either 'shrinkage' or 'independence' (default: [])</span>
0021 <span class="comment">%   Kappa        : between-class covariance regularization parameter, reasonable range: 0:0.1:1, greater is stronger (default: [])</span>
0022 <span class="comment">%</span>
0023 <span class="comment">%   Options  : optional name-value parameters to control the training details:</span>
0024 <span class="comment">%              'weight_bias' -&gt; 0/1, take unequal class priors into account for bias calculation</span>
0025 <span class="comment">%                               default: 0 -- note: this parameter is currently ignored</span>
0026 <span class="comment">%              'weight_cov' -&gt; 0/1, take unequal class priors into account for covariance calculation</span>
0027 <span class="comment">%                              default: 0</span>
0028 <span class="comment">%              'regularization' -&gt; 'shrinkage': covariance shrinkage, depends on lambda</span>
0029 <span class="comment">%                                  'independence': feature independence, depends on lambda</span>
0030 <span class="comment">%                                  'auto': analytical covariance shrinkage, lambda is ignored (default)</span>
0031 <span class="comment">% Out:</span>
0032 <span class="comment">%   Model   : a quadratic model; q is the quadratic weights, l is the linear weights, b is the bias;</span>
0033 <span class="comment">%             classes indicates the class labels which the model predicts; sc_info holds scale parameters</span>
0034 <span class="comment">%</span>
0035 <span class="comment">% Examples:</span>
0036 <span class="comment">%   % learn a shrinkage QDA model</span>
0037 <span class="comment">%   model = ml_trainqda(trials,targets);</span>
0038 <span class="comment">%</span>
0039 <span class="comment">%   % take unequal class priors into account for both the bias and the covariance matrix</span>
0040 <span class="comment">%   model = ml_trainqda(trials,targets,[],[],'weight_bias',1,'weight_cov',1);</span>
0041 <span class="comment">%</span>
0042 <span class="comment">%   % learn a shrinkage QDA model which blends covariance estimates between classes, using a</span>
0043 <span class="comment">%   % regularization parameter</span>
0044 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'qda',[],search(0:0.1:1)}})</span>
0045 <span class="comment">%</span>
0046 <span class="comment">%   % use a different type of regularization, which controls feature independence</span>
0047 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),[],'regularization','independence'}})</span>
0048 <span class="comment">%</span>
0049 <span class="comment">%   % as before, but search for the optimal blending parameter at the same time (slow!)</span>
0050 <span class="comment">%   model = utl_searchmodel({trials,target},'args',{{'qda',search(0:0.1:1),search(0:0.1:1),'regularization','independence'}})</span>
0051 <span class="comment">%</span>
0052 <span class="comment">% See also:</span>
0053 <span class="comment">%   ml_predictqda</span>
0054 <span class="comment">%</span>
0055 <span class="comment">% References:</span>
0056 <span class="comment">%   [1] Friedman, J. &quot;Regularized discriminant analysis.&quot;</span>
0057 <span class="comment">%       Journal of the American Statistical Association 84, 405 (1989), 175, 165.</span>
0058 <span class="comment">%</span>
0059 <span class="comment">%                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0060 <span class="comment">%                           2010-04-03</span>
0061     
0062 arg_define([0 4],varargin, <span class="keyword">...</span>
0063     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0064     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0065     arg({<span class="string">'lambda'</span>,<span class="string">'Lambda'</span>}, [], [], <span class="string">'Within-class covariance regularization parameter. Reasonable range: 0:0.1:1 - greater is stronger. Requires that the regularization mode is set to either &quot;shrinkage&quot; or &quot;independence&quot;.'</span>), <span class="keyword">...</span>
0066     arg({<span class="string">'kappav'</span>,<span class="string">'Kappa'</span>,<span class="string">'kappa'</span>}, [], [], <span class="string">'Between-class covariance regularization parameter. Reasonable range: 0:0.1:1 - greater is stronger. Requires that the regularization mode is set to either &quot;shrinkage&quot; or &quot;independence&quot;.'</span>), <span class="keyword">...</span>
0067     arg({<span class="string">'regularization'</span>,<span class="string">'Regularizer'</span>}, <span class="string">'auto'</span>, {<span class="string">'auto'</span>,<span class="string">'shrinkage'</span>,<span class="string">'independence'</span>}, <span class="string">'Regularization type. Regularizes the robustness / flexibility of covariance estimates. Auto is analytical covariance shrinkage, shrinkage is shrinkage as selected via lambda, and independence is feature independence, also selected via lambda.'</span>), <span class="keyword">...</span>
0068     arg({<span class="string">'weight_bias'</span>,<span class="string">'WeightedBias'</span>}, false, [], <span class="string">'Account for class priors in bias. If you do have unequal probabilities for the different classes, this should be enabled.'</span>), <span class="keyword">...</span>
0069     arg({<span class="string">'weight_cov'</span>,<span class="string">'WeightedCov'</span>}, false, [], <span class="string">'Account for class priors in covariance. If you do have unequal probabilities for the different classes, it makes sense to enable this.'</span>));
0070 
0071 
0072 <span class="comment">% find the class labels</span>
0073 classes = unique(targets);
0074 <span class="keyword">if</span> length(classes) &gt; 2
0075     <span class="comment">% learn a voting arrangement</span>
0076     model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(trials,targets,<span class="string">'1vR'</span>,@<a href="ml_trainqda.html" class="code" title="function model = ml_trainqda(varargin)">ml_trainqda</a>,@<a href="ml_predictqda.html" class="code" title="function pred = ml_predictqda(trials, model)">ml_predictqda</a>,varargin{:}); <span class="comment">%#ok&lt;*NODEF&gt;</span>
0077 <span class="keyword">elseif</span> length(classes) == 1
0078     error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0079 <span class="keyword">else</span>
0080     <span class="comment">% pre-prune degenerate features</span>
0081     retain = true(1,size(trials,2));
0082     <span class="keyword">for</span> c = 1:length(classes)
0083         X = trials(targets==classes(c),:);
0084         n{c} = size(X,1);
0085         mu{c} = mean(X);
0086         v{c} = var(X);
0087         retain = retain &amp; isfinite(mu{c}) &amp; isfinite(v{c}) &amp; v{c} &gt; eps;
0088     <span class="keyword">end</span>    
0089     <span class="comment">% apply feature mask...</span>
0090     trials = trials(:,retain);
0091     
0092     <span class="comment">% scale the data (to keep the numerics happy)</span>
0093     sc_info = hlp_findscaling(trials,<span class="string">'std'</span>);
0094     trials = hlp_applyscaling(trials,sc_info);
0095         
0096     <span class="comment">% estimate distribution of each class...</span>
0097     <span class="keyword">for</span> c = 1:length(classes)
0098         <span class="comment">% get mean and covariance</span>
0099         X = trials(targets==classes(c),:);
0100         n{c} = size(X,1);
0101         mu{c} = mean(X);
0102         <span class="keyword">if</span> strcmp(regularization,<span class="string">'auto'</span>)
0103             sig{c} = cov_shrink(X);
0104         <span class="keyword">else</span>
0105             sig{c} = cov(X);
0106             <span class="keyword">if</span> ~isempty(lambda)
0107                 <span class="comment">% lambda-dependent regularization</span>
0108                 <span class="keyword">if</span> strcmp(regularization,<span class="string">'independence'</span>)
0109                     sig{c} = (1-lambda)*sig{c} + lambda * diag(diag(sig{c}));
0110                 <span class="keyword">elseif</span> strcmp(regularization,<span class="string">'shrinkage'</span>)
0111                     sig{c} = (1-lambda)*sig{c} + lambda*eye(length(sig{c}))*abs(mean(diag(sig{c})));
0112                 <span class="keyword">else</span>
0113                     error(<span class="string">'unknown regularization mode'</span>);
0114                 <span class="keyword">end</span>
0115             <span class="keyword">end</span>
0116         <span class="keyword">end</span>
0117     <span class="keyword">end</span>
0118     
0119     <span class="keyword">if</span> ~isempty(kappav)
0120         <span class="comment">% regularize the covariance matrices w.r.t. each other</span>
0121         ns = fastif(weight_cov,n,{1 1});
0122         sigma = (sig{1}*ns{1} + sig{2}*ns{2}) / (ns{1}+ns{2});
0123         sig{1} = sig{1} * (1-kappav) + sigma*kappav;
0124         sig{2} = sig{2} * (1-kappav) + sigma*kappav;
0125     <span class="keyword">end</span>
0126     
0127     <span class="comment">% compute the model</span>
0128     model = struct(<span class="string">'c'</span>,{1/2*(logdet(sig{1})-logdet(sig{2})) + 1/2*((mu{1}/sig{1})*mu{1}' - (mu{2}/sig{2})*mu{2}')}, <span class="keyword">...</span>
0129         <span class="string">'l'</span>,{mu{1}/sig{1} - mu{2}/sig{2}}, <span class="string">'q'</span>,{-1/2*(inv(sig{1}) - inv(sig{2}))}, <span class="string">'sc_info'</span>,{sc_info}, <span class="string">'classes'</span>,{classes}, <span class="string">'featuremask'</span>,{retain});
0130 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 20-Aug-2013 03:44:10 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>