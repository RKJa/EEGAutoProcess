<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainhkl</title>
  <meta name="keywords" content="ml_trainhkl">
  <meta name="description" content="Learn a sparse non-linear predictive model using Hierarchical Kernel Learning.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainhkl.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainhkl
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a sparse non-linear predictive model using Hierarchical Kernel Learning.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainhkl(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a sparse non-linear predictive model using Hierarchical Kernel Learning.
 Model = ml_trainhkl(Trials, Targets, Lambdas, Options...)

 Hiararchical Kernel Learning [1,2] is a non-linear, kernel-based (as in Support Vector Machines)
 learning algorithm which can be used both for regression and classification. It can give
 probabilistic outputs (similar to logistic regression). The defining property of HKL is that it
 can select relevant features out of a large number of potentially many irrelevant ones (similar to
 LASSO), whereas these features are selected according to non-linear criteria, which makes HKL
 uniquely powerful in selecting relevant portions of the data.

 The method is relatively slow due to the need for regularization, but on the other hand has the
 highest chance of learning relevant structure in very large numbers of dimensions (among the
 provided machine learning methods). For these reasons, it is not the best method to start with, as
 others allow to evaluate data and to iterate through parameters at a much faster rate. It is,
 however, one of the best methods to try in order to get the best possible results, and also to
 check whether data on which other methods have failed contains any information of interest at all.
 

 In:
   Trials       : training data, as in ml_train

   Targets      : target variable, as in ml_train

   Lambdas      : vector of regularization parameters to optimize for
                  default: 10.^[1:-.5:-8]; note: if this is specified as a vector instead of a search() expression,
                  ml_trainhkl will internally optimize the parameter using a more efficient procedure; search() can nevertheless
                  be used instead, to get finer control over cross-validation (e.g. safety margins, etc.)

   Options  : optional name-value pairs to control the training details:
              'loss'  : either 'logistic' for classification or 'squared' for regression (default: 'logistic')
              'kernel': type of kernel to use; can be one of the following;
                        'hermite': hermite polynomial kernel (default)
                        'spline' spline kernel 
                        'gauss-hermite-full':  hermite expansion of the Gaussian kernel (finite order)
                        'anova': ANOVA kernel
                        'polynomial': polynomial kernel (not recommended)
              'params': vector of kernel parameters; if empty, chosen according to the author's recommendations (default: [])
                        see hkl.m for a detailed explanation of this parameter
              'mingap': result precision; expressed as duality gap (default: 1e-3)
              'gapeta': smoothing of the reduced problem (default: 1e-3)
              'maxactive': maximum number of selected kernel (default: 100)
              'display': display diagnostic outputs (default: 0)
              'data_normalization': either 'center' (for centering) or 'scale' (for centering and standardization) (default: 'scale')
              'kernel_normalization': 'center' (for centering) 'scale' (for centering and standardization), or 'scale-root' (default: 'scale-root')
              'alpha': dual parameter, see [2]
              'eta': kernel weights, see [2]
              'b': constant term, see [2]
              'memory_cache': size of the memory cache; should be large enough to fill most of the free RAM (default: 1e9)
              'k': foldness of an internal cross-validation, to compute the optimal regularization parameter (default: 5)

 Out:
   Model   : a HKL model...

 See also:
   <a href="ml_predicthkl.html" class="code" title="function pred = ml_predicthkl(trials, model)">ml_predicthkl</a>

 Examples:
   % learn a standard HKL classifier
   model = ml_trainhkl(trials,targets);

   % as before, but use a specific set of regularization values
   model = ml_trainhkl(trials,targets.10.^[1:-1:-8]);

   % like before, but use the squared loss for regression
   model = ml_trainhkl(trials,targets.10.^[1:-1:-8],'loss','squared');

   % like before, but this time use a different kernel type
   model = ml_trainhkl(trials,targets.10.^[1:-1:-8],'kernel','spline');


 References:
   [1] F. Bach. &quot;Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning.&quot;
       Advances in Neural Information Processing Systems (NIPS), 2008.
   [2] F. Bach. &quot;High-Dimensional Non-Linear Variable Selection through Hierarchical Kernel Learning.&quot;
       Technical report, HAL 00413473, 2009.

                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                           2010-04-04</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_predicthkl.html" class="code" title="function pred = ml_predicthkl(trials, model)">ml_predicthkl</a>	Prediction function for Hierarchical Kernel Learning.</li><li><a href="ml_trainhkl.html" class="code" title="function model = ml_trainhkl(varargin)">ml_trainhkl</a>	Learn a sparse non-linear predictive model using Hierarchical Kernel Learning.</li><li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_trainhkl.html" class="code" title="function model = ml_trainhkl(varargin)">ml_trainhkl</a>	Learn a sparse non-linear predictive model using Hierarchical Kernel Learning.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainhkl(varargin)</a>
0002 <span class="comment">% Learn a sparse non-linear predictive model using Hierarchical Kernel Learning.</span>
0003 <span class="comment">% Model = ml_trainhkl(Trials, Targets, Lambdas, Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% Hiararchical Kernel Learning [1,2] is a non-linear, kernel-based (as in Support Vector Machines)</span>
0006 <span class="comment">% learning algorithm which can be used both for regression and classification. It can give</span>
0007 <span class="comment">% probabilistic outputs (similar to logistic regression). The defining property of HKL is that it</span>
0008 <span class="comment">% can select relevant features out of a large number of potentially many irrelevant ones (similar to</span>
0009 <span class="comment">% LASSO), whereas these features are selected according to non-linear criteria, which makes HKL</span>
0010 <span class="comment">% uniquely powerful in selecting relevant portions of the data.</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% The method is relatively slow due to the need for regularization, but on the other hand has the</span>
0013 <span class="comment">% highest chance of learning relevant structure in very large numbers of dimensions (among the</span>
0014 <span class="comment">% provided machine learning methods). For these reasons, it is not the best method to start with, as</span>
0015 <span class="comment">% others allow to evaluate data and to iterate through parameters at a much faster rate. It is,</span>
0016 <span class="comment">% however, one of the best methods to try in order to get the best possible results, and also to</span>
0017 <span class="comment">% check whether data on which other methods have failed contains any information of interest at all.</span>
0018 <span class="comment">%</span>
0019 <span class="comment">%</span>
0020 <span class="comment">% In:</span>
0021 <span class="comment">%   Trials       : training data, as in ml_train</span>
0022 <span class="comment">%</span>
0023 <span class="comment">%   Targets      : target variable, as in ml_train</span>
0024 <span class="comment">%</span>
0025 <span class="comment">%   Lambdas      : vector of regularization parameters to optimize for</span>
0026 <span class="comment">%                  default: 10.^[1:-.5:-8]; note: if this is specified as a vector instead of a search() expression,</span>
0027 <span class="comment">%                  ml_trainhkl will internally optimize the parameter using a more efficient procedure; search() can nevertheless</span>
0028 <span class="comment">%                  be used instead, to get finer control over cross-validation (e.g. safety margins, etc.)</span>
0029 <span class="comment">%</span>
0030 <span class="comment">%   Options  : optional name-value pairs to control the training details:</span>
0031 <span class="comment">%              'loss'  : either 'logistic' for classification or 'squared' for regression (default: 'logistic')</span>
0032 <span class="comment">%              'kernel': type of kernel to use; can be one of the following;</span>
0033 <span class="comment">%                        'hermite': hermite polynomial kernel (default)</span>
0034 <span class="comment">%                        'spline' spline kernel</span>
0035 <span class="comment">%                        'gauss-hermite-full':  hermite expansion of the Gaussian kernel (finite order)</span>
0036 <span class="comment">%                        'anova': ANOVA kernel</span>
0037 <span class="comment">%                        'polynomial': polynomial kernel (not recommended)</span>
0038 <span class="comment">%              'params': vector of kernel parameters; if empty, chosen according to the author's recommendations (default: [])</span>
0039 <span class="comment">%                        see hkl.m for a detailed explanation of this parameter</span>
0040 <span class="comment">%              'mingap': result precision; expressed as duality gap (default: 1e-3)</span>
0041 <span class="comment">%              'gapeta': smoothing of the reduced problem (default: 1e-3)</span>
0042 <span class="comment">%              'maxactive': maximum number of selected kernel (default: 100)</span>
0043 <span class="comment">%              'display': display diagnostic outputs (default: 0)</span>
0044 <span class="comment">%              'data_normalization': either 'center' (for centering) or 'scale' (for centering and standardization) (default: 'scale')</span>
0045 <span class="comment">%              'kernel_normalization': 'center' (for centering) 'scale' (for centering and standardization), or 'scale-root' (default: 'scale-root')</span>
0046 <span class="comment">%              'alpha': dual parameter, see [2]</span>
0047 <span class="comment">%              'eta': kernel weights, see [2]</span>
0048 <span class="comment">%              'b': constant term, see [2]</span>
0049 <span class="comment">%              'memory_cache': size of the memory cache; should be large enough to fill most of the free RAM (default: 1e9)</span>
0050 <span class="comment">%              'k': foldness of an internal cross-validation, to compute the optimal regularization parameter (default: 5)</span>
0051 <span class="comment">%</span>
0052 <span class="comment">% Out:</span>
0053 <span class="comment">%   Model   : a HKL model...</span>
0054 <span class="comment">%</span>
0055 <span class="comment">% See also:</span>
0056 <span class="comment">%   ml_predicthkl</span>
0057 <span class="comment">%</span>
0058 <span class="comment">% Examples:</span>
0059 <span class="comment">%   % learn a standard HKL classifier</span>
0060 <span class="comment">%   model = ml_trainhkl(trials,targets);</span>
0061 <span class="comment">%</span>
0062 <span class="comment">%   % as before, but use a specific set of regularization values</span>
0063 <span class="comment">%   model = ml_trainhkl(trials,targets.10.^[1:-1:-8]);</span>
0064 <span class="comment">%</span>
0065 <span class="comment">%   % like before, but use the squared loss for regression</span>
0066 <span class="comment">%   model = ml_trainhkl(trials,targets.10.^[1:-1:-8],'loss','squared');</span>
0067 <span class="comment">%</span>
0068 <span class="comment">%   % like before, but this time use a different kernel type</span>
0069 <span class="comment">%   model = ml_trainhkl(trials,targets.10.^[1:-1:-8],'kernel','spline');</span>
0070 <span class="comment">%</span>
0071 <span class="comment">%</span>
0072 <span class="comment">% References:</span>
0073 <span class="comment">%   [1] F. Bach. &quot;Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning.&quot;</span>
0074 <span class="comment">%       Advances in Neural Information Processing Systems (NIPS), 2008.</span>
0075 <span class="comment">%   [2] F. Bach. &quot;High-Dimensional Non-Linear Variable Selection through Hierarchical Kernel Learning.&quot;</span>
0076 <span class="comment">%       Technical report, HAL 00413473, 2009.</span>
0077 <span class="comment">%</span>
0078 <span class="comment">%                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0079 <span class="comment">%                           2010-04-04</span>
0080 
0081 opts=arg_define([0 3],varargin, <span class="keyword">...</span>
0082     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0083     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0084     arg({<span class="string">'lambdas'</span>,<span class="string">'Lambdas'</span>}, 10.^(1:-.5:-8), [], <span class="string">'Vector of regularization parameters to consider. If this is specified as a vector instead of a search() expression, HKL will internally optimize the parameter using a more efficient procedure than the framework''s search() facility.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0085     arg({<span class="string">'loss'</span>,<span class="string">'LossFunction'</span>}, <span class="string">'logistic'</span>, {<span class="string">'logistic'</span>,<span class="string">'squared'</span>}, <span class="string">'Loss function to be used. The logistic loss is suited for classification problems, whereas the squared loss is suited for regression problems.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0086     arg({<span class="string">'kernel'</span>,<span class="string">'KernelFunc'</span>}, <span class="string">'hermite'</span>, {<span class="string">'hermite'</span>,<span class="string">'spline'</span>,<span class="string">'gauss-hermite-full'</span>,<span class="string">'anova'</span>,<span class="string">'polynomial'</span>}, <span class="string">'Class of kernels to use. Hermite polynomials, splines, Hermite expansion of the Gaussian kernel, ANOVA kernel, or the (not recommended) polynomial kernel.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0087     arg({<span class="string">'params'</span>,<span class="string">'KernelParams'</span>}, [], [], <span class="string">'Vector of kernel parameters. If empty, chosen according to the author''s recommendations (Francis Bach).'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0088     arg({<span class="string">'mingap'</span>,<span class="string">'Precision'</span>}, 1e-3, [], <span class="string">'Desired precision. Expressed as the duality gap.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0089     arg({<span class="string">'gapeta'</span>,<span class="string">'GapETA'</span>},1e-3,[],<span class="string">'Smoothing degree of the reduced problem.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0090     <span class="keyword">...</span>
0091     arg({<span class="string">'maxactive'</span>,<span class="string">'MaxKernelsActive'</span>},400,[],<span class="string">'Maximum active kernels..'</span>,<span class="string">'cat'</span>,<span class="string">'Efficiency'</span>), <span class="keyword">...</span>
0092     arg({<span class="string">'memory_cache'</span>,<span class="string">'MemoryCache'</span>},0.25,[],<span class="string">'Size of the memory cache. If &lt; 1, assumed to be a fraction of the current free memory, otherwise in bytes. Should be large enough to fill most of the free RAM for efficiency.'</span>,<span class="string">'cat'</span>,<span class="string">'Efficiency'</span>), <span class="keyword">...</span>
0093     arg({<span class="string">'k'</span>,<span class="string">'Foldness'</span>},5,[],<span class="string">'Cross-validation folds for parameter search.'</span>,<span class="string">'cat'</span>,<span class="string">'Efficiency'</span>), <span class="keyword">...</span>
0094     <span class="keyword">...</span>
0095     arg({<span class="string">'showit'</span>,<span class="string">'Verbose'</span>,<span class="string">'display'</span>},true,[],<span class="string">'Display diagnostic output.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0096     arg({<span class="string">'data_normaliation'</span>,<span class="string">'DataNormalization'</span>},<span class="string">'center'</span>,{<span class="string">'center'</span>,<span class="string">'scale'</span>},<span class="string">'Data normalization. Very important if the data is not naturally normalized. Note: scale both centers and standardizes.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0097     arg({<span class="string">'kernel_normaliation'</span>,<span class="string">'KernelNormalization'</span>},<span class="string">'scale-root'</span>,{<span class="string">'center'</span>,<span class="string">'scale'</span>,<span class="string">'scale-root'</span>},<span class="string">'Normalization in the kernel space.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0098     arg({<span class="string">'alphaparam'</span>,<span class="string">'Alpha'</span>,<span class="string">'alpha'</span>},[],[],<span class="string">'Dual parameter. See the HAL 00413473 TechReport.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0099     arg({<span class="string">'eta'</span>,<span class="string">'Eta'</span>},[],[],<span class="string">'Kernel Weights. See the HAL 00413473 TechReport.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0100     arg({<span class="string">'b'</span>,<span class="string">'B'</span>},[],[],<span class="string">'Constant Term. See the HAL 00413473 TechReport.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>));
0101 
0102 arg_toworkspace(opts);
0103 
0104 <span class="keyword">if</span> isempty(lambdas)
0105     lambdas = 10.^(1:-.5:-8); <span class="keyword">end</span>
0106 
0107 <span class="keyword">if</span> iscell(targets) <span class="comment">%#ok&lt;*NODEF&gt;</span>
0108     targets = targets{1};
0109     disp(<span class="string">'note: hkl does not support weighted learning.'</span>);
0110 <span class="keyword">end</span>
0111 
0112 <span class="comment">% identify and remap the classes, if necessary</span>
0113 classes = unique(targets);
0114 <span class="keyword">if</span> strcmp(loss,<span class="string">'logistic'</span>)
0115     <span class="keyword">if</span> length(classes) &gt; 2
0116         <span class="comment">% in this case we use the voter...</span>
0117         model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(trials,targets,<span class="string">'1v1'</span>,@<a href="ml_trainhkl.html" class="code" title="function model = ml_trainhkl(varargin)">ml_trainhkl</a>,@<a href="ml_predicthkl.html" class="code" title="function pred = ml_predicthkl(trials, model)">ml_predicthkl</a>,varargin{:});
0118         <span class="keyword">return</span>;
0119     <span class="keyword">elseif</span> length(classes) == 1
0120         error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0121     <span class="keyword">else</span>
0122         <span class="comment">% remap targets</span>
0123         targets(targets == classes(1)) = 0;
0124         targets(targets == classes(2)) = 1;
0125     <span class="keyword">end</span>
0126 <span class="keyword">end</span>
0127 
0128 <span class="comment">% select default kernel parameter, if necessary</span>
0129 params = hlp_rewrite(kernel,<span class="string">'polynomial'</span>,[3 .1 4], <span class="string">'hermite'</span>,[.5 3 .1 4], <span class="string">'gauss-hermite'</span>,[1 .05 3 .1 .5], <span class="string">'gauss-hermite-full'</span>,[1 .05 3 .1 .5 30], <span class="string">'anova'</span>,[.05 .1 4 30], <span class="string">'spline'</span>,[.1 4 40]);
0130 
0131 
0132 <span class="comment">% turn into fraction of free memory</span>
0133 <span class="keyword">if</span> opts.memory_cache &lt; 1
0134     opts.memory_cache = hlp_memfree * opts.memory_cache; <span class="keyword">end</span>
0135 
0136 <span class="comment">% learn</span>
0137 warning off MATLAB:illConditionedMatrix
0138 args = hlp_struct2varargin(opts,<span class="string">'rewrite'</span>,{<span class="string">'alphaparam'</span>,<span class="string">'alpha'</span>},<span class="string">'restrict'</span>, {<span class="string">'mingap'</span>,<span class="string">'gapeta'</span>,<span class="string">'maxactive'</span>,<span class="string">'data_normalization'</span>,<span class="string">'kernel_normalization'</span>,<span class="string">'alpha'</span>,<span class="string">'eta'</span>,<span class="string">'b'</span>,<span class="string">'memory_cache'</span>,<span class="string">'conjgrad'</span>});
0139 <span class="keyword">if</span> length(lambdas)&gt;1
0140     <span class="comment">% find the best lambda by randomized CV</span>
0141     <span class="keyword">if</span> ~showit
0142         evalc(<span class="string">'[model.outputs,model.model,dum1,dum2,model.bestlambda] = hkl_kfold(k,''same'',trials,targets,lambdas,loss,kernel,params,args{:});'</span>);
0143     <span class="keyword">else</span>
0144         [model.outputs,model.model,dum1,dum2,model.bestlambda] = hkl_kfold(k,<span class="string">'same'</span>,trials,targets,lambdas,loss,kernel,params,args{:}); <span class="comment">%#ok&lt;ASGLU&gt;</span>
0145     <span class="keyword">end</span>
0146 <span class="keyword">else</span>
0147     <span class="comment">% compute the model directly for the given lambda</span>
0148     [model.outputs,model.model] = hlp_diskcache(<span class="string">'predictivemodels'</span>,@hkl,trials,targets,lambdas,loss,kernel,params,args{:});
0149     model.bestlambda = 1;
0150 <span class="keyword">end</span>
0151 
0152 model.classes = classes;
0153 model.loss = loss;</pre></div>
<hr><address>Generated on Tue 20-Aug-2013 03:44:10 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>