<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainsvmperf</title>
  <meta name="keywords" content="ml_trainsvmperf">
  <meta name="description" content="Learn a linear or non-linear predictive model by Support Vector Machines, using SVMperf.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainsvmperf.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainsvmperf
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a linear or non-linear predictive model by Support Vector Machines, using SVMperf.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainsvmperf(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a linear or non-linear predictive model by Support Vector Machines, using SVMperf.
 Model = ml_trainsvmperf(Trials, Targets, Cost, Options...)

 SVMperf [1] is a compehensive package providing linear and non-linear (kernel) Support Vector
 Machines [2] to do classification. Among the provided SVM implementations, it is the preferred
 method whenever more than just mis-classification rate is reqired as optimization criterion (more
 precisely: as loss function).

 Support Vector Machines are available for various machine learning tasks (SVMperf supports
 classification, SVMlight additionally supports regression and ranking), and are generally an
 excellent (fast and/or robust, depending on regularization choice) solution. Kernel SVMs are a
 special variant which allow for state-of-the-art non-linear learning (depending on the kernel
 type, the kernel parameter needs to be searched, in addition to the cost parameter). The major
 drawback of support vector machines is that they do not produce useful probability outputs, which
 restricts their use in decision-making setups. The other drawback is the need for regularization,
 which can be complicated and time-consuming (e.g. incurring nested cross-validation).
 
 When the data is known to require a non-linear classifier (or regressor), Support Vector Machines,
 Relevance Vector Machines (ml_trainrvm) and Hierarchical Kernel Learning (ml_trainhkl) are
 currently the only methods provided in the toolbox, and their major difference is whether they
 give probabilistic outputs and how much regularization they require (and how well they are able to
 pick out relevant information in high-dimensional feature spaces).

 SVMperf supports a variety of kernels (rbf being the most useful one in most cases) a variety of
 advanced loss functions, and various computational methods/variants that primarily differ in speed
 (depending on data size, number of features, etc.) [3,4,5,6]. SVMperf is not free for
 non-commercial uses.

 In:
   Trials   : training data, as in ml_train

   Targets  : target variable, as in ml_train

   Cost     : regularization parameter, reasonable range: 2.^(-5:2:15), greater is stronger

   Options  : optional name-value parameters to control the training details
               'loss': Loss function to use (0..5,10)
                       * 0/'ZO'    Zero/one loss: 1 if vector of predictions contains error, 0 otherwise.
                       * 1/'F1'    F1: 100 minus the F1-score in percent.
                       * 2/'Err'   Errorrate: Percentage of errors in prediction vector.
                       * 3/'PRBEP' Prec/Rec Breakeven: 100 minus PRBEP in percent.
                       * 4/'Prec'  Prec@k: 100 minus precision at k in percent.
                       * 5/'Rec'   Rec@k: 100 minus recall at k in percent.
                       * 10/'ROC'  ROCArea: Percentage of swapped pos/neg pairs (i.e. 100 - ROCArea).
               'method': choice of structural learning algorithm (0..9), see arg declaration below for details
               'variant': CPSP variant for sparse kernel training (0,1,2,4, if method is 9), see arg declaration below for details
               'slacknorm': L-norm to use for slack variables (1,2)
               'loss_rescaling': Rescaling method to use for loss (1,2)
               'sparse_basis': number of basis functions for sparse kernel approximation (e.g. 500)
               'restarts': number of restarts during sparse kernel approximation (0+)
              kernel parameters:
               'kernel': ptype of kernel function (linear,poly,rbf,sigmoid,user); (default: 'rbf')
               'gamma': parameter gamma in rbf kernel; reasonable search range: 2.^(-16:2:4) (default: 0.3)
               'd': parameter d in polynomial kernel (default: 3)
               's': parameter s in sigmoid/poly kernel (default: 1)
               'r': parameter c in sigmoid/poly kernel (default: 1)
               'u': parameter of user-defined kernel (default: '1')
              misc options:
               'eps': tolerance (e.g., 0.1)
               'bias': bias present? (0,1, default:1)
               'pr_k': k in Precision/Recall loss functions
               'shrinking_heuristic': whether shrinking heuristic is used (0,1)
               'quiet': quiet mode (0,1)
               'scaling': pre-scaling, see hlp_findscaling (default: 'std')

 Out:
   Model   : a linear model; 
             classes indicates the class labels which the model predicts
             sc_info is the scaling info

 Notes:
   uses the SVM-struct learning module: SVM-perf, V3.00, 15.07.2009
     includes SVM-struct V3.10 for learning complex outputs, 14.08.08
     includes SVM-light V6.20 quadratic optimizer, 14.08.08
     SVM-perf, SVM-struct, SVM-light copyright (C) Thorsten Joachims, thorsten@joachims.org

 Examples:
   % learn a quick and dirty SVM model (without parameter search)
   model = ml_trainsvmperf(trials,labels)

   % learn an SVM model by searching over the cost parameter (note: quite slow)
   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15))}})

   % as before, but also search over the kernel scale parameter (note: really slow)
   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15)),'gamma',search(2.^(-16:2:4))}})

   % as before, but use F1 loss
   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15)),'loss','F1','gamma',search(2.^(-16:2:4))}})

   % as before, but use a linear kernel (no need to search over gamma, then)
   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15)),'kernel','linear'}})


 See also:
   <a href="ml_predictsvmperf.html" class="code" title="function pred = ml_predictsvmperf(trials, model)">ml_predictsvmperf</a>

 References:
   [1] T. Joachims, &quot;A Support Vector Method for Multivariate Performance Measures&quot;,
       Proceedings of the International Conference on Machine Learning (ICML), 2005.
   [2] Schoelkopf, B., and Smola, A. &quot;Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond&quot;
       (Adaptive Computation and Machine Learning). The MIT Press, Dec. 2001.
   [3] T. Joachims, &quot;Training Linear SVMs in Linear Time&quot;,
       Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2006.
   [4] T. Joachims, Chun-Nam Yu, &quot;Sparse Kernel SVMs via Cutting-Plane Training&quot;,
       Proceedings of the European Conference on Machine Learning (ECML), 2009.
   [5] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, &quot;Large Margin Methods for Structured and Interdependent Output Variables&quot;,
       Journal of Machine Learning Research (JMLR), Vol. 6(Sep):1453-1484, 2005.
   [6] T. Joachims, T. Finley, Chun-Nam Yu, &quot;Cutting-Plane Training of Structural SVMs&quot;,
       Machine Learning Journal, to appear.

                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                           2010-04-04</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_predictsvmperf.html" class="code" title="function pred = ml_predictsvmperf(trials, model)">ml_predictsvmperf</a>	Prediction function for the Support Vector Machine (SVMperf).</li><li><a href="ml_trainsvmperf.html" class="code" title="function model = ml_trainsvmperf(varargin)">ml_trainsvmperf</a>	Learn a linear or non-linear predictive model by Support Vector Machines, using SVMperf.</li><li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_trainsvmperf.html" class="code" title="function model = ml_trainsvmperf(varargin)">ml_trainsvmperf</a>	Learn a linear or non-linear predictive model by Support Vector Machines, using SVMperf.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainsvmperf(varargin)</a>
0002 <span class="comment">% Learn a linear or non-linear predictive model by Support Vector Machines, using SVMperf.</span>
0003 <span class="comment">% Model = ml_trainsvmperf(Trials, Targets, Cost, Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% SVMperf [1] is a compehensive package providing linear and non-linear (kernel) Support Vector</span>
0006 <span class="comment">% Machines [2] to do classification. Among the provided SVM implementations, it is the preferred</span>
0007 <span class="comment">% method whenever more than just mis-classification rate is reqired as optimization criterion (more</span>
0008 <span class="comment">% precisely: as loss function).</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% Support Vector Machines are available for various machine learning tasks (SVMperf supports</span>
0011 <span class="comment">% classification, SVMlight additionally supports regression and ranking), and are generally an</span>
0012 <span class="comment">% excellent (fast and/or robust, depending on regularization choice) solution. Kernel SVMs are a</span>
0013 <span class="comment">% special variant which allow for state-of-the-art non-linear learning (depending on the kernel</span>
0014 <span class="comment">% type, the kernel parameter needs to be searched, in addition to the cost parameter). The major</span>
0015 <span class="comment">% drawback of support vector machines is that they do not produce useful probability outputs, which</span>
0016 <span class="comment">% restricts their use in decision-making setups. The other drawback is the need for regularization,</span>
0017 <span class="comment">% which can be complicated and time-consuming (e.g. incurring nested cross-validation).</span>
0018 <span class="comment">%</span>
0019 <span class="comment">% When the data is known to require a non-linear classifier (or regressor), Support Vector Machines,</span>
0020 <span class="comment">% Relevance Vector Machines (ml_trainrvm) and Hierarchical Kernel Learning (ml_trainhkl) are</span>
0021 <span class="comment">% currently the only methods provided in the toolbox, and their major difference is whether they</span>
0022 <span class="comment">% give probabilistic outputs and how much regularization they require (and how well they are able to</span>
0023 <span class="comment">% pick out relevant information in high-dimensional feature spaces).</span>
0024 <span class="comment">%</span>
0025 <span class="comment">% SVMperf supports a variety of kernels (rbf being the most useful one in most cases) a variety of</span>
0026 <span class="comment">% advanced loss functions, and various computational methods/variants that primarily differ in speed</span>
0027 <span class="comment">% (depending on data size, number of features, etc.) [3,4,5,6]. SVMperf is not free for</span>
0028 <span class="comment">% non-commercial uses.</span>
0029 <span class="comment">%</span>
0030 <span class="comment">% In:</span>
0031 <span class="comment">%   Trials   : training data, as in ml_train</span>
0032 <span class="comment">%</span>
0033 <span class="comment">%   Targets  : target variable, as in ml_train</span>
0034 <span class="comment">%</span>
0035 <span class="comment">%   Cost     : regularization parameter, reasonable range: 2.^(-5:2:15), greater is stronger</span>
0036 <span class="comment">%</span>
0037 <span class="comment">%   Options  : optional name-value parameters to control the training details</span>
0038 <span class="comment">%               'loss': Loss function to use (0..5,10)</span>
0039 <span class="comment">%                       * 0/'ZO'    Zero/one loss: 1 if vector of predictions contains error, 0 otherwise.</span>
0040 <span class="comment">%                       * 1/'F1'    F1: 100 minus the F1-score in percent.</span>
0041 <span class="comment">%                       * 2/'Err'   Errorrate: Percentage of errors in prediction vector.</span>
0042 <span class="comment">%                       * 3/'PRBEP' Prec/Rec Breakeven: 100 minus PRBEP in percent.</span>
0043 <span class="comment">%                       * 4/'Prec'  Prec@k: 100 minus precision at k in percent.</span>
0044 <span class="comment">%                       * 5/'Rec'   Rec@k: 100 minus recall at k in percent.</span>
0045 <span class="comment">%                       * 10/'ROC'  ROCArea: Percentage of swapped pos/neg pairs (i.e. 100 - ROCArea).</span>
0046 <span class="comment">%               'method': choice of structural learning algorithm (0..9), see arg declaration below for details</span>
0047 <span class="comment">%               'variant': CPSP variant for sparse kernel training (0,1,2,4, if method is 9), see arg declaration below for details</span>
0048 <span class="comment">%               'slacknorm': L-norm to use for slack variables (1,2)</span>
0049 <span class="comment">%               'loss_rescaling': Rescaling method to use for loss (1,2)</span>
0050 <span class="comment">%               'sparse_basis': number of basis functions for sparse kernel approximation (e.g. 500)</span>
0051 <span class="comment">%               'restarts': number of restarts during sparse kernel approximation (0+)</span>
0052 <span class="comment">%              kernel parameters:</span>
0053 <span class="comment">%               'kernel': ptype of kernel function (linear,poly,rbf,sigmoid,user); (default: 'rbf')</span>
0054 <span class="comment">%               'gamma': parameter gamma in rbf kernel; reasonable search range: 2.^(-16:2:4) (default: 0.3)</span>
0055 <span class="comment">%               'd': parameter d in polynomial kernel (default: 3)</span>
0056 <span class="comment">%               's': parameter s in sigmoid/poly kernel (default: 1)</span>
0057 <span class="comment">%               'r': parameter c in sigmoid/poly kernel (default: 1)</span>
0058 <span class="comment">%               'u': parameter of user-defined kernel (default: '1')</span>
0059 <span class="comment">%              misc options:</span>
0060 <span class="comment">%               'eps': tolerance (e.g., 0.1)</span>
0061 <span class="comment">%               'bias': bias present? (0,1, default:1)</span>
0062 <span class="comment">%               'pr_k': k in Precision/Recall loss functions</span>
0063 <span class="comment">%               'shrinking_heuristic': whether shrinking heuristic is used (0,1)</span>
0064 <span class="comment">%               'quiet': quiet mode (0,1)</span>
0065 <span class="comment">%               'scaling': pre-scaling, see hlp_findscaling (default: 'std')</span>
0066 <span class="comment">%</span>
0067 <span class="comment">% Out:</span>
0068 <span class="comment">%   Model   : a linear model;</span>
0069 <span class="comment">%             classes indicates the class labels which the model predicts</span>
0070 <span class="comment">%             sc_info is the scaling info</span>
0071 <span class="comment">%</span>
0072 <span class="comment">% Notes:</span>
0073 <span class="comment">%   uses the SVM-struct learning module: SVM-perf, V3.00, 15.07.2009</span>
0074 <span class="comment">%     includes SVM-struct V3.10 for learning complex outputs, 14.08.08</span>
0075 <span class="comment">%     includes SVM-light V6.20 quadratic optimizer, 14.08.08</span>
0076 <span class="comment">%     SVM-perf, SVM-struct, SVM-light copyright (C) Thorsten Joachims, thorsten@joachims.org</span>
0077 <span class="comment">%</span>
0078 <span class="comment">% Examples:</span>
0079 <span class="comment">%   % learn a quick and dirty SVM model (without parameter search)</span>
0080 <span class="comment">%   model = ml_trainsvmperf(trials,labels)</span>
0081 <span class="comment">%</span>
0082 <span class="comment">%   % learn an SVM model by searching over the cost parameter (note: quite slow)</span>
0083 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15))}})</span>
0084 <span class="comment">%</span>
0085 <span class="comment">%   % as before, but also search over the kernel scale parameter (note: really slow)</span>
0086 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15)),'gamma',search(2.^(-16:2:4))}})</span>
0087 <span class="comment">%</span>
0088 <span class="comment">%   % as before, but use F1 loss</span>
0089 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15)),'loss','F1','gamma',search(2.^(-16:2:4))}})</span>
0090 <span class="comment">%</span>
0091 <span class="comment">%   % as before, but use a linear kernel (no need to search over gamma, then)</span>
0092 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svmperf',search(2.^(-5:2:15)),'kernel','linear'}})</span>
0093 <span class="comment">%</span>
0094 <span class="comment">%</span>
0095 <span class="comment">% See also:</span>
0096 <span class="comment">%   ml_predictsvmperf</span>
0097 <span class="comment">%</span>
0098 <span class="comment">% References:</span>
0099 <span class="comment">%   [1] T. Joachims, &quot;A Support Vector Method for Multivariate Performance Measures&quot;,</span>
0100 <span class="comment">%       Proceedings of the International Conference on Machine Learning (ICML), 2005.</span>
0101 <span class="comment">%   [2] Schoelkopf, B., and Smola, A. &quot;Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond&quot;</span>
0102 <span class="comment">%       (Adaptive Computation and Machine Learning). The MIT Press, Dec. 2001.</span>
0103 <span class="comment">%   [3] T. Joachims, &quot;Training Linear SVMs in Linear Time&quot;,</span>
0104 <span class="comment">%       Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2006.</span>
0105 <span class="comment">%   [4] T. Joachims, Chun-Nam Yu, &quot;Sparse Kernel SVMs via Cutting-Plane Training&quot;,</span>
0106 <span class="comment">%       Proceedings of the European Conference on Machine Learning (ECML), 2009.</span>
0107 <span class="comment">%   [5] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, &quot;Large Margin Methods for Structured and Interdependent Output Variables&quot;,</span>
0108 <span class="comment">%       Journal of Machine Learning Research (JMLR), Vol. 6(Sep):1453-1484, 2005.</span>
0109 <span class="comment">%   [6] T. Joachims, T. Finley, Chun-Nam Yu, &quot;Cutting-Plane Training of Structural SVMs&quot;,</span>
0110 <span class="comment">%       Machine Learning Journal, to appear.</span>
0111 <span class="comment">%</span>
0112 <span class="comment">%                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0113 <span class="comment">%                           2010-04-04</span>
0114 
0115 arg_define([0 3],varargin, <span class="keyword">...</span>
0116     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0117     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0118     arg({<span class="string">'cost'</span>,<span class="string">'Cost'</span>}, search(2.^(-5:2:15)), [], <span class="string">'Regularization parameter. Reasonable range: 2.^(-5:2:15), greater is stronger. By default, it is the (average of x*x)^-1.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0119     arg({<span class="string">'loss_function'</span>,<span class="string">'Loss'</span>,<span class="string">'loss'</span>}, <span class="string">'Err'</span>, {<span class="string">'Z0'</span>,<span class="string">'F1'</span>,<span class="string">'Err'</span>,<span class="string">'PRBEP'</span>,<span class="string">'Prec'</span>,<span class="string">'Rec'</span>,<span class="string">'ROC'</span>}, <span class="string">'Loss function to use. Zero/one loss, F1-score, error rate, Precision/Recall at break-even point, Precision at k, Recall at k, ROC area (percentage of swapped pos/neg pairs). Note that some losses x are actually 1-x (since their natural formulation is &quot;positive&quot; rather than in terms of loss).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0120     arg({<span class="string">'method'</span>,<span class="string">'LearningAlgo'</span>}, <span class="string">'Custom'</span>, {<span class="string">'n-Slack'</span>,<span class="string">'n-SlackShrink'</span>,<span class="string">'1-SlackPrimal'</span>,<span class="string">'1-SlackDual'</span>,<span class="string">'1-SlackDualCCache'</span>,<span class="string">'Custom'</span>}, <span class="string">'Structural learning algorithm. N-slack algorithm, n-slack algorithm with shrinking heuristic, 1-slack algorithm (primal), 1-slack algorithm (dual), 1-slack algorithm (dual) with constraint cache, or the custom algorithm (see machine_learning/ml_trainsvmperf for references).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0121     arg({<span class="string">'variant'</span>,<span class="string">'Method9CPSP'</span>}, <span class="string">'FixedPoint'</span>, {<span class="string">'off'</span>,<span class="string">'SubsetHeurstic'</span>,<span class="string">'FixedPoint'</span>,<span class="string">'FixedPointSubsetHeuristic'</span>}, <span class="string">'Variant of the CPSP algorithm. For sparse kernel training; Either do not use CPSP, or CPSP using subset selection for preimages via 59/95 heuristic, or CPSP using fixed point search (RBF kernel only), or CPSP using fixed point search with starting point via 59/95 heuristic (RBF kernel only).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0122     arg({<span class="string">'kernel'</span>,<span class="string">'Kernel'</span>}, <span class="string">'rbf'</span>, {<span class="string">'linear'</span>,<span class="string">'rbf'</span>,<span class="string">'poly'</span>,<span class="string">'sigmoid'</span>,<span class="string">'user'</span>}, <span class="string">'Kernel type. Linear, or Non-linear kernel types: Radial Basis Functions (general-purpose),  Polynomial (rarely preferred), Sigmoid (usually overly simple), User (user-defined kernel from kernel.h).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0123     arg({<span class="string">'g'</span>,<span class="string">'RBFScale'</span>,<span class="string">'gamma'</span>}, search(2.^(-16:2:4)), [], <span class="string">'Scaling parameter of the RBF kernel. Should match the size of structures in the data; A reasonable range is 2.^(-16:2:4).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0124     <span class="keyword">...</span>
0125     arg({<span class="string">'d'</span>,<span class="string">'PolyDegree'</span>}, uint32(3), [], <span class="string">'Degree for the polynomial kernel.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0126     arg({<span class="string">'s'</span>,<span class="string">'SigmoidPolyScale'</span>}, 1, [], <span class="string">'Scale of sigmoid/polynomial kernel.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0127     arg({<span class="string">'r'</span>,<span class="string">'SigmoidPolyBias'</span>}, 1, [], <span class="string">'Bias of sigmoid / polynomial kernel.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0128     arg({<span class="string">'u'</span>,<span class="string">'UserParameter'</span>}, <span class="string">'1'</span>, [], <span class="string">'User-defined kernel parameter.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'type'</span>,<span class="string">'char'</span>,<span class="string">'shape'</span>,<span class="string">'row'</span>), <span class="keyword">...</span>
0129     arg({<span class="string">'pr_k'</span>,<span class="string">'LossK'</span>}, uint32(0), [], <span class="string">'Fraction of positive examples k in Prec@k and Rec@k. Zero indicates to use 0.5x for Prec@k and 2x for Rec@k, with x being the number of positive examples in the training set.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0130     arg({<span class="string">'slacknorm'</span>,<span class="string">'SlackNorm'</span>}, <span class="string">'l1'</span>, {<span class="string">'l1'</span>,<span class="string">'l2'</span>}, <span class="string">'L-norm to use for slack variables. Either L1-norm, or squared slacks.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0131     arg({<span class="string">'loss_rescaling'</span>,<span class="string">'LossRescaling'</span>}, <span class="string">'margin'</span>, {<span class="string">'slack'</span>,<span class="string">'margin'</span>}, <span class="string">'Rescaling method to use for loss.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0132     arg({<span class="string">'sparse_basis'</span>,<span class="string">'NumBasisFunc'</span>}, uint32(500), [], <span class="string">'Mumber of basis functions. For sparse kernel approximation.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0133     arg({<span class="string">'restarts'</span>,<span class="string">'NumRestarts'</span>}, 0, [], <span class="string">'Number of restarts. Number of times to recompute the sparse kernel approximation and restart the optimizer.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0134     arg({<span class="string">'shrinking_heuristic'</span>,<span class="string">'ShrinkingHeuristic'</span>}, true, [], <span class="string">'Use shrinking heuristic in custom aglorithm. Only for linear kernel and errorrate loss.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>,<span class="string">'guru'</span>,true), <span class="keyword">...</span>
0135     arg({<span class="string">'scaling'</span>,<span class="string">'Scaling'</span>}, <span class="string">'std'</span>, {<span class="string">'none'</span>,<span class="string">'center'</span>,<span class="string">'std'</span>,<span class="string">'minmax'</span>,<span class="string">'whiten'</span>}, <span class="string">'Pre-scaling of the data. For the regulariation to work best, the features should either be naturally scaled well, or be artificially scaled.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0136     arg({<span class="string">'epsi'</span>,<span class="string">'Epsilon'</span>,<span class="string">'eps'</span>}, 0.1, [], <span class="string">'Tolerated solution accuracy.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0137     arg({<span class="string">'bias'</span>,<span class="string">'Bias'</span>}, false, [], <span class="string">'Include a bias term. Only implemented for linear kernel.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>), <span class="keyword">...</span>
0138     arg({<span class="string">'verbose'</span>,<span class="string">'Verbose'</span>}, false, [], <span class="string">'Show diagnostic output.'</span>,<span class="string">'cat'</span>,<span class="string">'Miscellaneous'</span>));
0139 
0140 <span class="keyword">if</span> is_search(cost)
0141     cost = 1; <span class="keyword">end</span>
0142 <span class="keyword">if</span> is_search(g)
0143     g = 0.3; <span class="keyword">end</span>
0144 
0145 <span class="comment">% scale the data</span>
0146 sc_info = hlp_findscaling(trials,scaling); <span class="comment">%#ok&lt;*NODEF&gt;</span>
0147 trials = hlp_applyscaling(trials,sc_info);
0148 
0149 <span class="comment">% find the class labels</span>
0150 classes = unique(targets);
0151 <span class="keyword">if</span> length(classes) &gt; 2
0152     <span class="comment">% in this case we use the voter</span>
0153     model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(trials,targets,<span class="string">'1v1'</span>,@<a href="ml_trainsvmperf.html" class="code" title="function model = ml_trainsvmperf(varargin)">ml_trainsvmperf</a>,@<a href="ml_predictsvmperf.html" class="code" title="function pred = ml_predictsvmperf(trials, model)">ml_predictsvmperf</a>,varargin{:});
0154 <span class="keyword">elseif</span> length(classes) == 1
0155     error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0156 <span class="keyword">else</span>
0157     
0158     <span class="comment">% remap target labels to -1,+1</span>
0159     targets(targets==classes(1)) = -1;
0160     targets(targets==classes(2)) = +1;
0161 
0162     <span class="comment">% rewrite sme string args to numbers</span>
0163     loss_function = hlp_rewrite(loss_function,<span class="string">'ZO'</span>,0,<span class="string">'F1'</span>,1,<span class="string">'Err'</span>,2,<span class="string">'PRBEP'</span>,3,<span class="string">'Prec'</span>,4,<span class="string">'Rec'</span>,5,<span class="string">'ROC'</span>,10);
0164     method = hlp_rewrite(method,<span class="string">'n-Slack'</span>,0,<span class="string">'n-SlackShrink'</span>,1,<span class="string">'1-SlackPrimal'</span>,2,<span class="string">'1-SlackDual'</span>,3,<span class="string">'1-SlackDualCCache'</span>,4,<span class="string">'Custom'</span>,9);
0165     kernel = hlp_rewrite(kernel,<span class="string">'linear'</span>,0,<span class="string">'poly'</span>,1,<span class="string">'rbf'</span>,2,<span class="string">'sigmoid'</span>,3,<span class="string">'user'</span>,4);
0166     variant = hlp_rewrite(variant,<span class="string">'off'</span>,0,<span class="string">'SubsetHeurstic'</span>,1,<span class="string">'FixedPoint'</span>,2,<span class="string">'FixedPointSubsetHeuristic'</span>,4);
0167     slacknorm = hlp_rewrite(slacknorm,<span class="string">'l1'</span>,1,<span class="string">'l2'</span>,2);
0168     loss_rescaling = hlp_rewrite(loss_rescaling,<span class="string">'slack'</span>,1,<span class="string">'margin'</span>,2);
0169 
0170     <span class="comment">% build the args</span>
0171     args = sprintf(<span class="string">'-c %f -v %d -y %d -p %d -o %d -l %d -w %d -e %f -t %d --p %d --k %d --r %d --s %d -d %d -g %f -s %f -r %f -u %s --b %d --i %d'</span>, <span class="keyword">...</span>
0172         cost,verbose,verbose,slacknorm,loss_rescaling,loss_function,method,epsi,kernel,pr_k,sparse_basis,restarts,shrinking_heuristic,d,g,s,r,u,bias,variant); <span class="comment">% note: verbose deliberately shows up twice</span>
0173 
0174     <span class="comment">% run the command</span>
0175     <span class="keyword">try</span>
0176         model = hlp_diskcache(<span class="string">'predictivemodels'</span>,@svmperflearn,trials,targets,args);
0177     <span class="keyword">catch</span> e
0178         <span class="keyword">if</span> strcmp(e.identifier,<span class="string">'MATLAB:UndefinedFunction'</span>)
0179             error(<span class="string">'The SVMperf library has not been compiled for your platform. Please consider using one of the other SVM implementations instead.'</span>); 
0180         <span class="keyword">else</span>
0181             rethrow(e);
0182         <span class="keyword">end</span>
0183     <span class="keyword">end</span>
0184     model.sc_info = sc_info;
0185     model.classes = classes;
0186 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 20-Aug-2013 03:44:10 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>