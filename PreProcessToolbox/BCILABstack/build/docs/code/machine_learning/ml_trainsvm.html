<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_trainsvm</title>
  <meta name="keywords" content="ml_trainsvm">
  <meta name="description" content="Learn a predictive model by Support Vector Machines.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_trainsvm.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_trainsvm
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Learn a predictive model by Support Vector Machines.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function model = ml_trainsvm(varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Learn a predictive model by Support Vector Machines.
 Model = ml_trainsvm(Trials, Targets, Cost, Options...)

 Support vector machines are nowadays some of the most frequently used and versatile linear
 classifiers [1]. They can handle a large number of feature efficiently and are usually extremely
 robust (if regularized well) or fast (when not regularized). The main differences between the
 provided variants are computational, and allow to select the most efficient model for the data (in
 terms of #trials/#features). All variants except for 'native' are implemented using the LIBLINEAR
 package [2], the 'native' variant servers as fallback (in case LIBLINEAR is not available for some
 platform). Support vector machines do not naturally produce good probability estimates (although
 some approach exists, like in the LDA case).

 in typical uses of the toolbox, SVMs are not as frequently used as logreg or lda, primarily due to
 the need for proper regularization, which is often prohibitively costly. For quick tests, the
 method can be used without regularization, however. When extremely many feature or trials are
 used, SVMs are likely more useful than in standard settings.

 In:
   Trials       : training data, as in ml_train

   Targets      : target variable, as in ml_train

   Cost         : regularization parameter, reasonable range: 2.^(-5:2:15), greater is stronger
                  default: 1

   Options  : optional name-value pairs; possible names are:
              'variant': one of several SVM variants
                       'dual'    : L2-regularized L2-loss support vector classification (dual), usually preferred (default)
                       'primal'  : L2-regularized L2-loss support vector classification (primal), can be faster than dual
                       'crammer' : Crammer and Singer approach to multi-class support vector classification,
                                   as alternative to the voted classification employed in the other variants
                       'sparse'  : L1-regularized L2-loss support vector classification, gives sparse results but
                                   will likely not be better unless the solution is truly sparse.
                       'l1loss'  : L2-regularized L1-loss support vector classification (dual), rarely needed
                       'native'  : native MATLAB implementation (using CVX); equivalent to 'dual', but slower

              'kernel': one of several kernel types:
                         * 'linear':   Linear
                         * 'rbf':      Gaussian / radial basis functions (default)
                         * 'laplace':  Laplacian
                         * 'poly':        Polynomial
                         * 'cauchy':    Cauchy

              'gamma': scaling parameter of the kernel (for regularization), default: 0.3
                        reasonable range: 2.^(-16:2:4)

              'degree': degree of the polynomial kernel, if used (default: 3)

              'eps'    : desired accuracy (default: [], meaning LIBLINEAR's default)

              'bias'   : include a bias variable (default: 1)

              'scaling': pre-scaling of the data (see hlp_findscaling for options) (default: 'std') 

              'verbose'  : verbosity level (default: 0)

 Out:
   Model   : a predictive model; 
             classes indicates the class labels which the model predicts
             sc_info is the scaling info

 Examples:
   % learn a quick and dirty SVM classifier (without parameter search)
   model = ml_trainsvm(trials,labels)


   % learn an SVM model by searching over the cost parameter (note: quite slow)
   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15))}})

   % as before, but also search over the kernel scale parameter (note: really slow)
   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'gamma',search(2.^(-16:2:4))}})

   % as before, but use a linear kernel (no need to search over gamma in this case)
   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'kernel','linear'}})

   % as before, but use primal optimization; this is faster under certain circumstances
   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'variant','primal','kernel','linear'}})

   % as before, but use the native MATLAB implementation; this may be used when the binary code 
   % does not run for some reason (note: should better be run on a cluster, as this is extremely slow)
   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'variant','native','kernel','linear'}})


 See also:
   <a href="ml_predictsvm.html" class="code" title="function pred = ml_predictsvm(trials, model)">ml_predictsvm</a>

 References:
   [1] P. S. Bradley and O. L. Mangasarian. &quot;Massive data discrimination via linear support vector machines.&quot;
       Optimization Methods and Software, 13:1-10, 2000.
   [2] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. &quot;LIBLINEAR: A library for large linear classification&quot;
       Journal of Machine Learning Research 9(2008), 1871-1874.

                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                           2010-04-04</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_predictsvm.html" class="code" title="function pred = ml_predictsvm(trials, model)">ml_predictsvm</a>	Prediction function for the Support Vector Machine.</li><li><a href="ml_trainsvm.html" class="code" title="function model = ml_trainsvm(varargin)">ml_trainsvm</a>	Learn a predictive model by Support Vector Machines.</li><li><a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>	Internal meta-algorithm for voting. Used by other machine learning functions.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="ml_trainsvm.html" class="code" title="function model = ml_trainsvm(varargin)">ml_trainsvm</a>	Learn a predictive model by Support Vector Machines.</li></ul>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function model = ml_trainsvm(varargin)</a>
0002 <span class="comment">% Learn a predictive model by Support Vector Machines.</span>
0003 <span class="comment">% Model = ml_trainsvm(Trials, Targets, Cost, Options...)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% Support vector machines are nowadays some of the most frequently used and versatile linear</span>
0006 <span class="comment">% classifiers [1]. They can handle a large number of feature efficiently and are usually extremely</span>
0007 <span class="comment">% robust (if regularized well) or fast (when not regularized). The main differences between the</span>
0008 <span class="comment">% provided variants are computational, and allow to select the most efficient model for the data (in</span>
0009 <span class="comment">% terms of #trials/#features). All variants except for 'native' are implemented using the LIBLINEAR</span>
0010 <span class="comment">% package [2], the 'native' variant servers as fallback (in case LIBLINEAR is not available for some</span>
0011 <span class="comment">% platform). Support vector machines do not naturally produce good probability estimates (although</span>
0012 <span class="comment">% some approach exists, like in the LDA case).</span>
0013 <span class="comment">%</span>
0014 <span class="comment">% in typical uses of the toolbox, SVMs are not as frequently used as logreg or lda, primarily due to</span>
0015 <span class="comment">% the need for proper regularization, which is often prohibitively costly. For quick tests, the</span>
0016 <span class="comment">% method can be used without regularization, however. When extremely many feature or trials are</span>
0017 <span class="comment">% used, SVMs are likely more useful than in standard settings.</span>
0018 <span class="comment">%</span>
0019 <span class="comment">% In:</span>
0020 <span class="comment">%   Trials       : training data, as in ml_train</span>
0021 <span class="comment">%</span>
0022 <span class="comment">%   Targets      : target variable, as in ml_train</span>
0023 <span class="comment">%</span>
0024 <span class="comment">%   Cost         : regularization parameter, reasonable range: 2.^(-5:2:15), greater is stronger</span>
0025 <span class="comment">%                  default: 1</span>
0026 <span class="comment">%</span>
0027 <span class="comment">%   Options  : optional name-value pairs; possible names are:</span>
0028 <span class="comment">%              'variant': one of several SVM variants</span>
0029 <span class="comment">%                       'dual'    : L2-regularized L2-loss support vector classification (dual), usually preferred (default)</span>
0030 <span class="comment">%                       'primal'  : L2-regularized L2-loss support vector classification (primal), can be faster than dual</span>
0031 <span class="comment">%                       'crammer' : Crammer and Singer approach to multi-class support vector classification,</span>
0032 <span class="comment">%                                   as alternative to the voted classification employed in the other variants</span>
0033 <span class="comment">%                       'sparse'  : L1-regularized L2-loss support vector classification, gives sparse results but</span>
0034 <span class="comment">%                                   will likely not be better unless the solution is truly sparse.</span>
0035 <span class="comment">%                       'l1loss'  : L2-regularized L1-loss support vector classification (dual), rarely needed</span>
0036 <span class="comment">%                       'native'  : native MATLAB implementation (using CVX); equivalent to 'dual', but slower</span>
0037 <span class="comment">%</span>
0038 <span class="comment">%              'kernel': one of several kernel types:</span>
0039 <span class="comment">%                         * 'linear':   Linear</span>
0040 <span class="comment">%                         * 'rbf':      Gaussian / radial basis functions (default)</span>
0041 <span class="comment">%                         * 'laplace':  Laplacian</span>
0042 <span class="comment">%                         * 'poly':        Polynomial</span>
0043 <span class="comment">%                         * 'cauchy':    Cauchy</span>
0044 <span class="comment">%</span>
0045 <span class="comment">%              'gamma': scaling parameter of the kernel (for regularization), default: 0.3</span>
0046 <span class="comment">%                        reasonable range: 2.^(-16:2:4)</span>
0047 <span class="comment">%</span>
0048 <span class="comment">%              'degree': degree of the polynomial kernel, if used (default: 3)</span>
0049 <span class="comment">%</span>
0050 <span class="comment">%              'eps'    : desired accuracy (default: [], meaning LIBLINEAR's default)</span>
0051 <span class="comment">%</span>
0052 <span class="comment">%              'bias'   : include a bias variable (default: 1)</span>
0053 <span class="comment">%</span>
0054 <span class="comment">%              'scaling': pre-scaling of the data (see hlp_findscaling for options) (default: 'std')</span>
0055 <span class="comment">%</span>
0056 <span class="comment">%              'verbose'  : verbosity level (default: 0)</span>
0057 <span class="comment">%</span>
0058 <span class="comment">% Out:</span>
0059 <span class="comment">%   Model   : a predictive model;</span>
0060 <span class="comment">%             classes indicates the class labels which the model predicts</span>
0061 <span class="comment">%             sc_info is the scaling info</span>
0062 <span class="comment">%</span>
0063 <span class="comment">% Examples:</span>
0064 <span class="comment">%   % learn a quick and dirty SVM classifier (without parameter search)</span>
0065 <span class="comment">%   model = ml_trainsvm(trials,labels)</span>
0066 <span class="comment">%</span>
0067 <span class="comment">%</span>
0068 <span class="comment">%   % learn an SVM model by searching over the cost parameter (note: quite slow)</span>
0069 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15))}})</span>
0070 <span class="comment">%</span>
0071 <span class="comment">%   % as before, but also search over the kernel scale parameter (note: really slow)</span>
0072 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'gamma',search(2.^(-16:2:4))}})</span>
0073 <span class="comment">%</span>
0074 <span class="comment">%   % as before, but use a linear kernel (no need to search over gamma in this case)</span>
0075 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'kernel','linear'}})</span>
0076 <span class="comment">%</span>
0077 <span class="comment">%   % as before, but use primal optimization; this is faster under certain circumstances</span>
0078 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'variant','primal','kernel','linear'}})</span>
0079 <span class="comment">%</span>
0080 <span class="comment">%   % as before, but use the native MATLAB implementation; this may be used when the binary code</span>
0081 <span class="comment">%   % does not run for some reason (note: should better be run on a cluster, as this is extremely slow)</span>
0082 <span class="comment">%   model = utl_searchmodel({trials,labels},'args',{{'svm',search(2.^(-5:2:15)),'variant','native','kernel','linear'}})</span>
0083 <span class="comment">%</span>
0084 <span class="comment">%</span>
0085 <span class="comment">% See also:</span>
0086 <span class="comment">%   ml_predictsvm</span>
0087 <span class="comment">%</span>
0088 <span class="comment">% References:</span>
0089 <span class="comment">%   [1] P. S. Bradley and O. L. Mangasarian. &quot;Massive data discrimination via linear support vector machines.&quot;</span>
0090 <span class="comment">%       Optimization Methods and Software, 13:1-10, 2000.</span>
0091 <span class="comment">%   [2] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. &quot;LIBLINEAR: A library for large linear classification&quot;</span>
0092 <span class="comment">%       Journal of Machine Learning Research 9(2008), 1871-1874.</span>
0093 <span class="comment">%</span>
0094 <span class="comment">%                           Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0095 <span class="comment">%                           2010-04-04</span>
0096 
0097 arg_define([0 3],varargin, <span class="keyword">...</span>
0098     arg_norep(<span class="string">'trials'</span>), <span class="keyword">...</span>
0099     arg_norep(<span class="string">'targets'</span>), <span class="keyword">...</span>
0100     arg({<span class="string">'cost'</span>,<span class="string">'Cost'</span>}, search(2.^(-5:2:15)), [], <span class="string">'Regularization parameter. Reasonable range: 2.^(-5:2:15), greater is stronger.'</span>), <span class="keyword">...</span>
0101     arg({<span class="string">'variant'</span>,<span class="string">'Variant'</span>}, <span class="string">'dual'</span>, {<span class="string">'dual'</span>,<span class="string">'primal'</span>,<span class="string">'crammer'</span>,<span class="string">'sparse'</span>,<span class="string">'l1loss'</span>,<span class="string">'native'</span>}, <span class="string">'Variant to use. Dual and primal are l2-regularized l2-loss support vector classification, dual usually preferred but primal can be faster. Crammer is a special approach to multi-class problems. Sparse assumes that the result should be sparse, l1loss uses a rarely used loss function, native is a native MATLAB implementation, if the binary fails to work.'</span>), <span class="keyword">...</span>
0102     arg({<span class="string">'kernel'</span>,<span class="string">'Kernel'</span>}, <span class="string">'rbf'</span>, {<span class="string">'linear'</span>,<span class="string">'rbf'</span>,<span class="string">'laplace'</span>,<span class="string">'poly'</span>,<span class="string">'cauchy'</span>}, <span class="string">'Kernel type. Linear, or Non-linear kernel types: Radial Basis Functions (general-purpose), Laplace (sparse), Polynomial (rarely preferred), and Cauchy (slightly experimental).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0103     arg({<span class="string">'gammap'</span>,<span class="string">'KernelScale'</span>,<span class="string">'gamma'</span>}, search(2.^(-16:2:4)), [], <span class="string">'Scaling of the kernel functions. Should match the size of structures in the data. A reasonable range is 2.^(-16:2:4).'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span>
0104     arg({<span class="string">'polydegree'</span>,<span class="string">'PolyDegree'</span>,<span class="string">'degree'</span>}, uint32(3), [], <span class="string">'Degree of the polynomial kernel, if chosen.'</span>,<span class="string">'cat'</span>,<span class="string">'Core Parameters'</span>), <span class="keyword">...</span><span class="comment">    </span>
0105     arg({<span class="string">'epsi'</span>,<span class="string">'Epsilon'</span>,<span class="string">'eps'</span>}, [], [], <span class="string">'Tolerated solution accuracy. If unspecified, the default of the library (LIBLINEAR) will be taken.'</span>), <span class="keyword">...</span>
0106     arg({<span class="string">'bias'</span>,<span class="string">'Bias'</span>}, true, [], <span class="string">'Include a bias term.'</span>), <span class="keyword">...</span>
0107     arg({<span class="string">'scaling'</span>,<span class="string">'Scaling'</span>}, <span class="string">'std'</span>, {<span class="string">'none'</span>,<span class="string">'center'</span>,<span class="string">'std'</span>,<span class="string">'minmax'</span>,<span class="string">'whiten'</span>}, <span class="string">'Pre-scaling of the data. For the regulariation to work best, the features should either be naturally scaled well, or be artificially scaled.'</span>), <span class="keyword">...</span>
0108     arg({<span class="string">'verbose'</span>,<span class="string">'Verbose'</span>}, false, [], <span class="string">'Whether to show diagnostic output.'</span>));
0109 
0110 <span class="keyword">if</span> is_search(cost)
0111     cost = 1; <span class="keyword">end</span>
0112 <span class="keyword">if</span> is_search(gammap)
0113     gammap = 0.3; <span class="keyword">end</span>
0114 
0115 
0116 <span class="comment">% identify and remap the classes (to 0..k)</span>
0117 classes = unique(targets); <span class="comment">%#ok&lt;*NODEF&gt;</span>
0118 <span class="keyword">if</span> strcmp(variant,<span class="string">'native'</span>)
0119     <span class="comment">% MATLAB version</span>
0120     <span class="keyword">if</span> length(classes) &gt; 2
0121         <span class="comment">% in this case we use the voter...</span>
0122         model = <a href="ml_trainvote.html" class="code" title="function model = ml_trainvote(trials, targets, votingscheme, learner, predictor, varargin)">ml_trainvote</a>(double(trials),targets,<span class="string">'1v1'</span>,@<a href="ml_trainsvm.html" class="code" title="function model = ml_trainsvm(varargin)">ml_trainsvm</a>,@<a href="ml_predictsvm.html" class="code" title="function pred = ml_predictsvm(trials, model)">ml_predictsvm</a>,varargin{:});
0123         <span class="keyword">return</span>;
0124     <span class="keyword">elseif</span> length(classes) == 1
0125         error(<span class="string">'BCILAB:only_one_class'</span>,<span class="string">'Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.'</span>);
0126     <span class="keyword">end</span>
0127     <span class="comment">% scale the data</span>
0128     sc_info = hlp_findscaling(trials,scaling);
0129     trials = hlp_applyscaling(trials,sc_info);
0130     <span class="comment">% kernelize the data</span>
0131     basis = trials;
0132     trials = utl_kernelize(trials,basis,kernel,gammap,polydegree);    
0133     <span class="comment">% remap targets</span>
0134     targets(targets == classes(1)) = -1;
0135     targets(targets == classes(2)) = +1;    
0136     <span class="comment">% solve</span>
0137     [n,f] = size(trials); <span class="comment">%#ok&lt;NASGU,ASGLU&gt;</span>
0138     cvx_begin
0139         variables w(f) b xi(n)
0140         minimize 1/2*sum(w.*w) + cost*sum(xi)
0141         subject to
0142             targets.*(trials*w + b) &gt;= 1 - xi;
0143             xi &gt;= 0;
0144     cvx_end
0145     model = struct(<span class="string">'w'</span>,w,<span class="string">'b'</span>,b);
0146 <span class="keyword">else</span>
0147     <span class="comment">% scale the data</span>
0148     sc_info = hlp_findscaling(trials,scaling);
0149     trials = hlp_applyscaling(trials,sc_info);        
0150     <span class="comment">% kernelize the data</span>
0151     basis = trials;
0152     trials = utl_kernelize(trials,basis,kernel,gammap,polydegree);
0153     <span class="comment">% remap targets</span>
0154     targ = targets;
0155     <span class="keyword">for</span> c=1:length(classes)
0156         targets(targ==classes(c)) = c-1; <span class="keyword">end</span>
0157     
0158     <span class="comment">% check whether we are using the fallback variant if LIBLINEAR</span>
0159     fallback = ~isempty(strfind(which(<span class="string">'lltrain'</span>),<span class="string">'pre7.3'</span>));
0160     <span class="keyword">if</span> fallback &amp;&amp; strcmp(variant,<span class="string">'sparse'</span>)
0161         error(<span class="string">'Sparse classifiers are not supported in LIBLINEAR prior to MATLAB 7.3 (due to a breaking change in the sparse array MEX API).'</span>); <span class="keyword">end</span>    
0162     
0163     <span class="comment">% rewrite some args</span>
0164     variant = hlp_rewrite(variant,<span class="string">'dual'</span>,1,<span class="string">'primal'</span>,2,<span class="string">'crammer'</span>,4,<span class="string">'l1loss'</span>,3,<span class="string">'sparse'</span>,5);
0165     bias = hlp_rewrite(bias,true,<span class="string">'-B 1'</span>,false,<span class="string">''</span>);
0166     quiet = hlp_rewrite(~verbose &amp;&amp; ~fallback,true,<span class="string">'-q'</span>,false,<span class="string">''</span>);
0167     
0168     <span class="comment">% build the arguments</span>
0169     <span class="keyword">if</span> ~isempty(epsi)
0170         args = sprintf(<span class="string">'-s %d -c %f -e %f %s %s'</span>,variant,cost,epsi,quiet,bias);
0171     <span class="keyword">else</span>
0172         args = sprintf(<span class="string">'-s %d -c %f %s %s'</span>,variant,cost,quiet,bias);
0173     <span class="keyword">end</span>
0174     <span class="comment">% run the command</span>
0175     model = hlp_diskcache(<span class="string">'predictivemodels'</span>,@llwtrain,ones(size(targets)),targets,sparse(double(trials)),args);
0176 <span class="keyword">end</span>
0177 <span class="comment">% finalize the model</span>
0178 model.sc_info = sc_info;
0179 model.classes = classes;
0180 model.variant = variant;
0181 model.basis = basis;
0182 model.kernel = kernel;
0183 model.gammap = gammap;
0184 model.degree = polydegree;</pre></div>
<hr><address>Generated on Tue 20-Aug-2013 03:44:10 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>